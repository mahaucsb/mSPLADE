{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6600a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, evaluation, InputExample\n",
    "from sentence_transformers import  models as smodel\n",
    "import models \n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import argparse\n",
    "import losses\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "424ce953",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Luyu/co-condenser-marco\"\n",
    "max_seq_length = 256\n",
    "word_embedding_model = models.MLMTransformer(model_name, max_seq_length=max_seq_length)\n",
    "for param in word_embedding_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "query_layer = smodel.Dense(in_features=word_embedding_model.get_word_embedding_dimension(), out_features=word_embedding_model.get_word_embedding_dimension())\n",
    "model = SentenceTransformer(modules=[word_embedding_model, query_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76742d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "808731it [01:39, 8168.24it/s] \n"
     ]
    }
   ],
   "source": [
    "num_negs_per_system = 5\n",
    "data_folder = '../msmarco'\n",
    "\n",
    "#### Read the corpus file containing all the passages. Store them in the corpus dict\n",
    "corpus = {}  # dict in the format: passage_id -> passage. Stores all existing passages\n",
    "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
    "if not os.path.exists(collection_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download collection.tar.gz\")\n",
    "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "logging.info(\"Read corpus: collection.tsv\")\n",
    "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\"\\t\")\n",
    "        pid = int(pid)\n",
    "        corpus[pid] = passage\n",
    "\n",
    "### Read the train queries, store in queries dict\n",
    "queries = {}  # dict in the format: query_id -> query. Stores all training queries\n",
    "queries_filepath = os.path.join(data_folder, 'queries.train.tsv')\n",
    "if not os.path.exists(queries_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download queries.tar.gz\")\n",
    "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, query = line.strip().split(\"\\t\")\n",
    "        qid = int(qid)\n",
    "        queries[qid] = query\n",
    "\n",
    "# Load a dict (qid, pid) -> ce_score that maps query-ids (qid) and paragraph-ids (pid)\n",
    "# to the CrossEncoder score computed by the cross-encoder-ms-marco-MiniLM-L-6-v2 model\n",
    "ce_scores_file = os.path.join(data_folder, 'cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz')\n",
    "if not os.path.exists(ce_scores_file):\n",
    "    logging.info(\"Download cross-encoder scores file\")\n",
    "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz', ce_scores_file)\n",
    "\n",
    "logging.info(\"Load CrossEncoder scores dict\")\n",
    "with gzip.open(ce_scores_file, 'rb') as fIn:\n",
    "    ce_scores = pickle.load(fIn)\n",
    "\n",
    "# As training data we use hard-negatives that have been mined using various systems\n",
    "hard_negatives_filepath = os.path.join(data_folder, 'msmarco-hard-negatives.jsonl.gz')\n",
    "\n",
    "train_queries = {}\n",
    "negs_to_use = None\n",
    "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        data = json.loads(line)\n",
    "\n",
    "        #Get the positive passage ids\n",
    "        pos_pids = data['pos']\n",
    "\n",
    "        #Get the hard negatives\n",
    "        neg_pids = set()\n",
    "\n",
    "        negs_to_use = list(data['neg'].keys())\n",
    "           \n",
    "        for system_name in negs_to_use:\n",
    "            if system_name not in data['neg']:\n",
    "                continue\n",
    "\n",
    "            system_negs = data['neg'][system_name]\n",
    "            negs_added = 0\n",
    "            for pid in system_negs:\n",
    "                if pid not in neg_pids:\n",
    "                    neg_pids.add(pid)\n",
    "                    negs_added += 1\n",
    "                    if negs_added >= num_negs_per_system:\n",
    "                        break\n",
    "\n",
    "        if (len(pos_pids) > 0 and len(neg_pids) > 0):\n",
    "            train_queries[data['qid']] = {'qid': data['qid'], 'query': queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n",
    "\n",
    "logging.info(\"Train queries: {}\".format(len(train_queries)))\n",
    "\n",
    "# We create a custom MS MARCO dataset that returns triplets (query, positive, negative)\n",
    "# on-the-fly based on the information from the mined-hard-negatives jsonl file.\n",
    "class MSMARCODataset(Dataset):\n",
    "    def __init__(self, queries, corpus, ce_scores):\n",
    "        self.queries = queries\n",
    "        self.queries_ids = list(queries.keys())\n",
    "        self.corpus = corpus\n",
    "        self.ce_scores = ce_scores\n",
    "\n",
    "        for qid in self.queries:\n",
    "            self.queries[qid]['pos'] = list(self.queries[qid]['pos'])\n",
    "            self.queries[qid]['neg'] = list(self.queries[qid]['neg'])\n",
    "            random.shuffle(self.queries[qid]['neg'])\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        query = self.queries[self.queries_ids[item]]\n",
    "        query_text = query['query']\n",
    "        qid = query['qid']\n",
    "\n",
    "        if len(query['pos']) > 0:\n",
    "            pos_id = query['pos'].pop(0)    #Pop positive and add at end\n",
    "            pos_text = self.corpus[pos_id]\n",
    "            query['pos'].append(pos_id)\n",
    "        else:   #We only have negatives, use two negs\n",
    "            pos_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "            pos_text = self.corpus[pos_id]\n",
    "            query['neg'].append(pos_id)\n",
    "\n",
    "        #Get a negative passage\n",
    "        neg_id = query['neg'].pop(0)    #Pop negative and add at end\n",
    "        neg_text = self.corpus[neg_id]\n",
    "        query['neg'].append(neg_id)\n",
    "\n",
    "        pos_score = self.ce_scores[qid][pos_id]\n",
    "        neg_score = self.ce_scores[qid][neg_id]\n",
    "\n",
    "        return InputExample(texts=[query_text, pos_text, neg_text], label=pos_score-neg_score)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d61e703a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from typing import Iterable, Dict\n",
    "\n",
    "class UNIFORM: \n",
    "    def __call__(self, x, t=2):\n",
    "        result = torch.pdist(x, p=2).pow(2).mul(-t)\n",
    "        return result.exp().mean().log()\n",
    "\n",
    "\n",
    "class MarginMSELossSplade(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute the MSE loss between the |sim(Query, Pos) - sim(Query, Neg)| and |gold_sim(Q, Pos) - gold_sim(Query, Neg)|\n",
    "    By default, sim() is the dot-product\n",
    "    For more details, please refer to https://arxiv.org/abs/2010.02666\n",
    "    \"\"\"\n",
    "    def __init__(self, model, similarity_fct = losses.pairwise_dot_score, lambda_d=8e-2, lambda_q=1e-1, lambda_uni = 1e-2, uni_mse = False, uni_d = False, uni_q = False):\n",
    "        \"\"\"\n",
    "        :param model: SentenceTransformerModel\n",
    "        :param similarity_fct:  Which similarity function to use\n",
    "        \"\"\"\n",
    "        super(MarginMSELossSplade, self).__init__()\n",
    "        self.model = model\n",
    "        self.similarity_fct = similarity_fct\n",
    "        self.loss_fct = nn.MSELoss()\n",
    "        self.lambda_d = lambda_d\n",
    "        self.lambda_q = lambda_q\n",
    "        self.FLOPS = losses.FLOPS()\n",
    "        self.uniform_mse = uni_mse\n",
    "        self.uni_d = uni_d\n",
    "        self.uni_q = uni_q\n",
    "        self.uni = UNIFORM()\n",
    "        self.lambda_uni = lambda_uni\n",
    "\n",
    "    def forward(self, sentence_features: Iterable[Dict[str, Tensor]], labels: Tensor):\n",
    "        # sentence_features: query, positive passage, negative passage\n",
    "        embeddings_query = self.model(sentence_features[0])['sentence_embedding'] \n",
    "        embeddings_pos = self.model[0](sentence_features[1])['sentence_embedding'] \n",
    "        embeddings_neg = self.model[0](sentence_features[2])['sentence_embedding'] \n",
    "        \n",
    "        scores_pos = self.similarity_fct(embeddings_query, embeddings_pos)\n",
    "        scores_neg = self.similarity_fct(embeddings_query, embeddings_neg)\n",
    "        margin_pred = scores_pos - scores_neg\n",
    "        \n",
    "        flops_doc = self.lambda_d*(self.FLOPS(embeddings_pos) + self.FLOPS(embeddings_neg))\n",
    "        flops_query = self.lambda_q*(self.FLOPS(embeddings_query))\n",
    "        overall_loss = self.loss_fct(margin_pred, labels)\n",
    "        print(\"0\", overall_loss)\n",
    "        \n",
    "        uni_d = self.uni(torch.nn.functional.normalize(embeddings_pos,dim=1))\n",
    "        \n",
    "        uni_q = self.uni(torch.nn.functional.normalize(embeddings_query,dim=1))\n",
    "        \n",
    "        if self.uni_d:\n",
    "           overall_loss +=  self.lambda_uni * uni_d\n",
    "        if self.uni_q:\n",
    "           overall_loss +=  self.lambda_uni * uni_q\n",
    "        if self.uniform_mse:\n",
    "            uniform_dist = self.lambda_uni * (uni_q - uni_d) ** 2\n",
    "            overall_loss +=  uniform_dist\n",
    "        print(\"1\", (uni_q - uni_d) ** 2)\n",
    "        return overall_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd1aca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MSMARCODataset(queries=train_queries, corpus=corpus, ce_scores=ce_scores)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, drop_last=True)\n",
    "train_loss = MarginMSELossSplade(model=model, lambda_d=0.01, lambda_q=0.01, lambda_uni = 1, uni_mse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e18eff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0389, -0.0761, -0.0617,  ..., -0.0482, -0.0597, -0.0184],\n",
      "        [-0.0442, -0.0332,  0.0141,  ..., -0.0077, -0.0159, -0.0392],\n",
      "        [-0.0403, -0.0679,  0.0146,  ...,  0.0318, -0.0262,  0.0031],\n",
      "        ...,\n",
      "        [-0.0263, -0.0573, -0.0065,  ...,  0.0549, -0.0242, -0.0405],\n",
      "        [-0.0189, -0.0761,  0.0150,  ..., -0.0028, -0.0224, -0.0069],\n",
      "        [ 0.0124, -0.0798, -0.0331,  ..., -0.0126, -0.0744,  0.0577]],\n",
      "       device='cuda:0')\n",
      "Parameter containing:\n",
      "tensor([[-0.0009,  0.0047, -0.0018,  ..., -0.0022,  0.0025,  0.0033],\n",
      "        [ 0.0003, -0.0012,  0.0020,  ...,  0.0041, -0.0033,  0.0020],\n",
      "        [-0.0048, -0.0025, -0.0032,  ..., -0.0009,  0.0015, -0.0030],\n",
      "        ...,\n",
      "        [-0.0045, -0.0041, -0.0045,  ..., -0.0052, -0.0020,  0.0024],\n",
      "        [ 0.0013, -0.0004,  0.0050,  ...,  0.0008, -0.0010,  0.0002],\n",
      "        [ 0.0049,  0.0028,  0.0020,  ..., -0.0037,  0.0011,  0.0028]],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for k in model[0].parameters():\n",
    "    print(k)\n",
    "    break\n",
    "    \n",
    "for k in model[1].parameters():\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42544b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8226b97eec754f649a4e4fb1ec2a2c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364057ea23f84add80099b030995358d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/251469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(154.4012, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "1 tensor(0.2839, device='cuda:0', grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 22.20 GiB total capacity; 16.20 GiB already allocated; 2.31 GiB free; 17.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23612/3949894959.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.fit(train_objectives=[(train_dataloader, train_loss)],\n\u001b[0m\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mwarmup_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0muse_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                         \u001b[0mscale_before_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splade/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/splade/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 22.20 GiB total capacity; 16.20 GiB already allocated; 2.31 GiB free; 17.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          epochs=20,\n",
    "          warmup_steps=10,\n",
    "          use_amp=True,\n",
    "          checkpoint_path=\".\",\n",
    "          checkpoint_save_steps=10000,\n",
    "          optimizer_params = {'lr':1e-4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5151b236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ef27fdbb8241c4865b662691708564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c707831129b54cda97f62c7ec84d2ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843231a5af2949e2a9d1d2e60cdfbd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af531e69e3254e34b8835c80df5bd965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d167b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receptor', 'and', '##rogen', 'define']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qtext = \"receptor androgen define\"\n",
    "tokenizer.tokenize(qtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7094b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "qrel_file = \"../../msmarco/qrels.train.tsv\"\n",
    "\n",
    "qrels = defaultdict(dict)\n",
    "with open(qrel_file) as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            qid, _, did, rel = line.strip().split(\"\\t\")\n",
    "        except:\n",
    "            qid, _, did, rel = line.strip().split(\" \")\n",
    "        if int(rel) > 0:\n",
    "            qrels[qid][did] = int(rel)\n",
    "        \n",
    "with open(\"../../msmarco/queries.train.tsv\") as f, open(\"../../msmarco/queries.train.wordpiece.tsv\", \"w\") as fo:\n",
    "    for line in f:\n",
    "        qid, qtext = line.strip().split(\"\\t\")\n",
    "        if qid in qrels:\n",
    "            fo.write(f\"{qid}\\t{' '.join(tokenizer.tokenize(qtext))}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862fe91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splade",
   "language": "python",
   "name": "splade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
