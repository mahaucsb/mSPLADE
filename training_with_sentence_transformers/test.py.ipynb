{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d4b5fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n",
      "2022-07-15 17:48:17 - Use pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "808731it [01:14, 10864.94it/s]\n"
     ]
    }
   ],
   "source": [
    "#FROM Sentence-BERT(https://github.com/UKPLab/sentence-transformers/blob/afee883a17ab039120783fd0cffe09ea979233cf/examples/training/ms_marco/train_bi-encoder_margin-mse.py) with minimal changes.\n",
    "#Original License Apache2, NOTE: Trained MSMARCO models are NonCommercial (from dataset License)\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, evaluation, InputExample\n",
    "from sbert import SentenceTransformerA\n",
    "import models\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import os\n",
    "import tarfile\n",
    "import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "from shutil import copyfile\n",
    "import pickle\n",
    "import argparse\n",
    "import losses\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from data import MSMARCODataset\n",
    "#from colbert_model import DETeacher\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "\n",
    "\n",
    "train_batch_size = 16  # Increasing the train batch size generally improves the model performance, but requires more GPU memory\n",
    "model_name = \"output/colbert_hn_[q][d]_num20_marginkldiv_position5-batch_size_16-2022-07-06_18-01-08/50000/0_ColBERTTransformer\"\n",
    "max_passages = 0\n",
    "ce_score_margin = 0\n",
    "max_seq_length = 256  # Max length for passages. Increasing it implies more GPU memory needed\n",
    "num_negs_per_system = 20 # We used different systems to mine hard negatives. Number of hard negatives to add from each system\n",
    "num_epochs = 10  # Number of epochs we want to train\n",
    "\n",
    "# Load our embedding model\n",
    "\n",
    "word_embedding_model = models.ColBERTTransformer(model_name, max_seq_length=max_seq_length)\n",
    "print(len(word_embedding_model.tokenizer))\n",
    "tokens = [\"[unused0]\", \"[unused1]\", \"[unused2]\"] #[unused0] for query, [unused1] for doc, [unused2] for query expansion\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "print(len(word_embedding_model.tokenizer))\n",
    "\n",
    "checkpoint = torch.load(os.path.join(model_name, \"checkpoint.pt\"), map_location='cpu')\n",
    "word_embedding_model.load_state_dict(checkpoint)\n",
    "\n",
    "model = SentenceTransformerA(modules=[word_embedding_model])\n",
    "model_save_path = f'output/test' #_prfspladedoc310\n",
    "\n",
    "# Write self to path\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "train_script_path = os.path.join(model_save_path, 'train_script.py')\n",
    "\n",
    "### Now we read the MS MARCO dataset\n",
    "\n",
    "data_folder = '../../msmarco'\n",
    "#data_folder = '/home/ec2-user/ebs/msmarco'\n",
    "\n",
    "\n",
    "#### Read the corpus file containing all the passages. Store them in the corpus dict\n",
    "corpus = {}  # dict in the format: passage_id -> passage. Stores all existing passages\n",
    "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
    "if not os.path.exists(collection_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'collection.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download collection.tar.gz\")\n",
    "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "\n",
    "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\"\\t\")\n",
    "        pid = int(pid)\n",
    "        corpus[pid] = passage\n",
    "\n",
    "### Read the train queries, store in queries dict\n",
    "queries = {}  # dict in the format: query_id -> query. Stores all training queries\n",
    "queries_filepath = os.path.join(data_folder, 'queries.train.tsv') #'queries.train.tsv'  'queries.train.qspladev2.top10.tsv'  'queries.train.spladedoc3_10.tsv'\n",
    "if not os.path.exists(queries_filepath):\n",
    "    tar_filepath = os.path.join(data_folder, 'queries.tar.gz')\n",
    "    if not os.path.exists(tar_filepath):\n",
    "        logging.info(\"Download queries.tar.gz\")\n",
    "        util.http_get('https://msmarco.blob.core.windows.net/msmarcoranking/queries.tar.gz', tar_filepath)\n",
    "\n",
    "    with tarfile.open(tar_filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_folder)\n",
    "\n",
    "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, query = line.strip().split(\"\\t\")\n",
    "        qid = int(qid)\n",
    "        queries[qid] = query\n",
    "\n",
    "\n",
    "ce_scores_file = os.path.join(data_folder, 'cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz')\n",
    "if not os.path.exists(ce_scores_file):\n",
    "    logging.info(\"Download cross-encoder scores file\")\n",
    "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz', ce_scores_file)\n",
    "\n",
    "with gzip.open(ce_scores_file, 'rb') as fIn:\n",
    "    ce_scores = pickle.load(fIn)\n",
    "\n",
    "# As training data we use hard-negatives that have been mined using various systems\n",
    "hard_negatives_filepath = os.path.join(data_folder, 'msmarco-hard-negatives-splade.jsonl.gz')\n",
    "if not os.path.exists(hard_negatives_filepath):\n",
    "    logging.info(\"Download cross-encoder scores file\")\n",
    "    util.http_get('https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/resolve/main/msmarco-hard-negatives.jsonl.gz', hard_negatives_filepath)\n",
    "\n",
    "\n",
    "train_queries = {}\n",
    "negs_to_use = None\n",
    "\n",
    "\n",
    "with gzip.open(hard_negatives_filepath, 'rt') as fIn:\n",
    "    for line in tqdm.tqdm(fIn):\n",
    "        if max_passages > 0 and len(train_queries) >= max_passages:\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "\n",
    "        #Get the positive passage ids\n",
    "        pos_pids = data['pos']\n",
    "\n",
    "        if len(pos_pids) == 0:  #Skip entries without positives passages\n",
    "            continue\n",
    "\n",
    "        pos_min_ce_score = min([ce_scores[data['qid']][pid] for pid in data['pos']])\n",
    "        ce_score_threshold = pos_min_ce_score - ce_score_margin\n",
    "\n",
    "        neg_pids = []\n",
    "        \n",
    "        #Get the hard negatives\n",
    "        \n",
    "        if negs_to_use is None:\n",
    "            negs_to_use = ['splade']\n",
    "            \n",
    "        for system_name in negs_to_use:\n",
    "            if system_name not in data['neg']:\n",
    "                continue\n",
    "\n",
    "            system_negs = data['neg'][system_name]\n",
    "            negs_added = 0\n",
    "            for pid in system_negs:\n",
    "                pid = int(pid)\n",
    "                if pid not in neg_pids:\n",
    "                    neg_pids.append(pid)\n",
    "                    negs_added += 1\n",
    "                    if negs_added >= num_negs_per_system:\n",
    "                        break\n",
    "\n",
    "        if ((len(pos_pids) > 0 and len(neg_pids) > 0)):\n",
    "            train_queries[data['qid']] = {'qid': data['qid'], 'query': queries[data['qid']], 'pos': pos_pids, 'neg': neg_pids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f16f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "The Jupyter notebook server failed to launch in time\n",
      "/home/lunwang/.conda/envs/splade/lib/python3.9/site-packages/traitlets/traitlets.py:2196: FutureWarning: Supporting extra quotes around Unicode is deprecated in traitlets 5.0. Use '/home/lunwang/DL/splade_cls/training_with_sentence_transformers' instead of '\"/home/lunwang/DL/splade_cls/training_with_sentence_transformers\"' – or use CUnicode.\n",
      "  warn(. \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "qmaping = defaultdict(list)\n",
    "all_did = set()\n",
    "with open(\"../../msmaro/effi.dev.trec\") as f:\n",
    "    for line in f:\n",
    "        qid, _, did, rank, splade_score , _ = line.split(\"\\t\")\n",
    "        qmapping[qid].append(did)\n",
    "        all_did.add(did)\n",
    "#### Read the corpus file containing all the passages. Store them in the corpus dict\n",
    "corpus = {}  # dict in the format: passage_id -> passage. Stores all existing passages\n",
    "data_folder = \"../../msmarco/\"\n",
    "collection_filepath = os.path.join(data_folder, 'collection.tsv')\n",
    "\n",
    "with open(collection_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        pid, passage = line.strip().split(\"\\t\")\n",
    "        pid = int(pid)\n",
    "        corpus[pid] = passage\n",
    "\n",
    "### Read the train queries, store in queries dict\n",
    "queries = {}  # dict in the format: query_id -> query. Stores all training queries\n",
    "queries_filepath = os.path.join(data_folder, 'queries.dev.tsv') #'queries.train.tsv'  'queries.train.qspladev2.top10.tsv'  'queries.train.spladedoc3_10.tsv'\n",
    "\n",
    "with open(queries_filepath, 'r', encoding='utf8') as fIn:\n",
    "    for line in fIn:\n",
    "        qid, query = line.strip().split(\"\\t\")\n",
    "        qid = int(qid)\n",
    "        queries[qid] = query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53a03c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for qid in train_queries:          \n",
    "    pos_list = []\n",
    "    for posid in train_queries[qid]['pos']:\n",
    "        if posid in train_queries[qid]['neg']:\n",
    "            pos_list.append([train_queries[qid]['neg'].index(posid) + 1, posid])\n",
    "        else:\n",
    "            pos_list.append([len(train_queries[qid]['neg']) + 1, posid])\n",
    "\n",
    "    target_scores = [[pid[1], ce_scores[qid][pid[1]]] for pid in pos_list] + [[pid, ce_scores[qid][pid]] for pid in train_queries[qid]['neg']]\n",
    "    target_scores = sorted(target_scores, key = lambda x: -x[1])\n",
    "    target_ids = [x[0] for x in target_scores]\n",
    "\n",
    "    train_queries[qid]['pos'] = [x + [target_ids.index(x[1]) + 1] for x in pos_list]\n",
    "    train_queries[qid]['neg'] = [[x[0] + 1, x[1], target_ids.index(x[1]) + 1] for x in enumerate(train_queries[qid]['neg'])]\n",
    "    train_queries[qid]['splade_pos'] = [x + [target_ids.index(x[1]) + 1] for x in pos_list]\n",
    "    train_queries[qid]['splade_neg'] = [x for x in train_queries[qid]['neg']]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "515b9455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442654061"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "open(\"output/colbert_g_num50_marginkldiv_position5-batch_size_16x2-2022-07-14_18-30-35/num0/train_queries.json\", \"w\").write(json.dumps(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b861357",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pytrec_eval\n",
    "from collections import OrderedDict, defaultdict\n",
    "from statistics import mean\n",
    "import json\n",
    "import sys\n",
    "from models import MLMTransformerDense, ColBERTTransformer\n",
    "from losses import pairwise_dot_score\n",
    "\n",
    "def _split_into_batches(features, bsize):\n",
    "    batches = []\n",
    "    for offset in range(0, features[\"input_ids\"].size(0), bsize):\n",
    "        batches.append({key: features[key][offset:offset+bsize] for key in features.keys()})\n",
    "\n",
    "    return batches\n",
    "agg = \"max\"\n",
    "bsize = 128\n",
    "\n",
    "\n",
    "def evaluate_trainining(model, tokenizer, train_queries, corpus, k1, k2):\n",
    "    model.eval()\n",
    "    for q in tqdm(train_queries):\n",
    "        dids = [x[1] for x in train_queries[q]['pos']] + [x[1] for x in train_queries[q]['neg']]\n",
    "        dtexts = [corpus[did] for did in dids]\n",
    "        with torch.no_grad():\n",
    "            q_features = tokenizer(\"[unused0] \" + train_queries[q]['query'], return_tensors=\"pt\").to('cuda')\n",
    "            q_features = model(q_features)\n",
    "            token_rep_q = torch.nn.functional.normalize(q_features['last_layer_embeddings'], p=2, dim=2)\n",
    "            \n",
    "            d_features = tokenizer([\"[unused1] \" + dtext for dtext in dtexts], return_tensors=\"pt\", max_length=256,truncation=True,padding=True)\n",
    "            d_features = _split_into_batches(d_features,bsize=bsize)\n",
    "            all_scores = []\n",
    "            for batch in d_features:\n",
    "                d_batch = model(batch)\n",
    "                d_mask = d_batch['attention_mask'].to('cuda')\n",
    "                d_emb = d_batch['last_layer_embeddings']\n",
    "                del d_batch\n",
    "                d_mask = d_mask.unsqueeze(-1)\n",
    "                token_rep_d =  d_emb * d_mask\n",
    "                del d_mask, d_emb\n",
    "                token_rep_d = torch.nn.functional.normalize(token_rep_d)\n",
    "                scores =  (token_rep_q @ token_rep_d.permute(0,2,1)).max(2).values.sum(1).tolist()\n",
    "                del token_rep_d\n",
    "                torch.cuda.empty_cache()\n",
    "                all_scores.extend(scores)\n",
    "\n",
    "            reranking_results = sorted([[did,score] for did, score in zip(dids, all_scores)], key = lambda x: -x[1])\n",
    "            #print(\"reranking\", reranking_results)\n",
    "            reranking_results = {x[1][0]: 1/(int(x[0]) + 1 + k2) for x in enumerate(reranking_results)}\n",
    "            #print(\"reranking\", reranking_results)\n",
    "            retrieval_results = {x[1]: 1/(int(x[0]) + k1) for x in train_queries[q]['splade_pos']} | {x[1]: 1/(int(x[0]) + k1) for x in train_queries[q]['splade_neg']}\n",
    "            #print(\"retrieval\", retrieval_results)\n",
    "            combined_results = []\n",
    "            for did in reranking_results:\n",
    "                combined_results.append([did, reranking_results[did] + retrieval_results[did]])\n",
    "            combined_results = sorted(combined_results, key = lambda x: -x[1])\n",
    "            #print(\"combined\", combined_results)\n",
    "            combined_lookup = dict()\n",
    "            for x in enumerate(combined_results):\n",
    "                combined_lookup[x[1][0]] = x[0] + 1\n",
    "            \n",
    "            pos_list = []\n",
    "            for x in train_queries[q]['pos']:\n",
    "                pos_list.append([combined_lookup[x[1]], x[1], x[2]])\n",
    "            \n",
    "            neg_list = []\n",
    "            for x in train_queries[q]['neg']:\n",
    "                neg_list.append([combined_lookup[x[1]], x[1], x[2]])\n",
    "            \n",
    "            train_queries[q]['pos'] = pos_list\n",
    "            train_queries[q]['neg'] = rneg_list\n",
    "            \n",
    "        \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e65e5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▍                                                                                                                                        | 1828/502937 [01:57<8:21:08, 16.67it/s]"
     ]
    }
   ],
   "source": [
    "evaluate_trainining(word_embedding_model.to('cuda'), word_embedding_model.tokenizer, train_queries, corpus, 60, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db216291",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': 571018,\n",
       " 'query': 'what are the liberal arts?',\n",
       " 'pos': [[8, 7349777, 2]],\n",
       " 'neg': [[1, 168975, 7],\n",
       "  [2, 1626275, 1],\n",
       "  [3, 168979, 5],\n",
       "  [4, 7349780, 6],\n",
       "  [5, 1065945, 4],\n",
       "  [6, 7179545, 8],\n",
       "  [7, 7179544, 9],\n",
       "  [8, 7349777, 2],\n",
       "  [9, 7349776, 10],\n",
       "  [10, 981824, 17],\n",
       "  [11, 6243359, 11],\n",
       "  [12, 4873670, 14],\n",
       "  [13, 7790851, 16],\n",
       "  [14, 2305472, 19],\n",
       "  [15, 6717931, 12],\n",
       "  [16, 1833474, 20],\n",
       "  [17, 7790847, 18],\n",
       "  [18, 7179539, 15],\n",
       "  [19, 4903324, 21],\n",
       "  [20, 168983, 13]],\n",
       " 'splade_pos': [[8, 7349777, 2]],\n",
       " 'splade_neg': [[1, 168975, 7],\n",
       "  [2, 1626275, 1],\n",
       "  [3, 168979, 5],\n",
       "  [4, 7349780, 6],\n",
       "  [5, 1065945, 4],\n",
       "  [6, 7179545, 8],\n",
       "  [7, 7179544, 9],\n",
       "  [8, 7349777, 2],\n",
       "  [9, 7349776, 10],\n",
       "  [10, 981824, 17],\n",
       "  [11, 6243359, 11],\n",
       "  [12, 4873670, 14],\n",
       "  [13, 7790851, 16],\n",
       "  [14, 2305472, 19],\n",
       "  [15, 6717931, 12],\n",
       "  [16, 1833474, 20],\n",
       "  [17, 7790847, 18],\n",
       "  [18, 7179539, 15],\n",
       "  [19, 4903324, 21],\n",
       "  [20, 168983, 13]]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_queries[571018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12c39cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../msmarco/queries.dev.expanded.10.tsv\") as f, open(\"../../msmarco/queries.dev.labeled.tsv\", \"w\") as fo:\n",
    "    for line in f:\n",
    "        fo.write(line.split(\" [SEP] [unused2] \")[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba325e22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('splade': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "28a80ffd683768ef022727de3d90aea96015029e3fc670f9e61f7ae03480e491"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
