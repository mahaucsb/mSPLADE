'''
Script  for  product CQ
'''
import torch
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter


# Quick utility function to sample from the Gumbel-distribution: -Log(-Log(Uniform)), eps to avoid numerical errors
def sample_gumbel(input_size, eps=1e-20):
    unif = torch.rand(input_size)

    return -torch.log(-torch.log(unif + eps) + eps)

class Encoder(nn.Module):
    '''
    The encoder takes embedding dimension, M, and K (from MxK coding scheme introduced in paper)
    as parameters. From a word's baseline embedding,
    it outputs Gumbel-softmax or one-hot encoding vectors d_w, reshaped for the decoder.
    In the original paper, the hidden_layer was fixed at M * K/2.

    Input shape: BATCH_SIZE X EMBEDDING_DIM
    Output shape: BATCH_SIZE X M X K X 1, Code (K X 1)

    '''
    def __init__(self, emb_size,M, K, hidden_size= None, position = False):
        super(Encoder, self).__init__()
        # input is cat[non-static, static]
        self.emb_size = emb_size
        self.K = K
        self.M = M
        # If not otherwise specified, use hidden_size indicated by paper
        if not hidden_size:
            #hidden_size = int(M * K / 2)
            hidden_size = 2048
        
        # This layer maps from the hidden layer to BATCH_SIZE X M K
        self.alpha_w = nn.Linear(hidden_size, M * K)
        self.position = position
        if position:
            # This linear layer maps to latent hidden representation
            self.h_w = nn.Linear(self.emb_size * 2 + 768, hidden_size)
        else:
            self.h_w = nn.Linear(self.emb_size * 2, hidden_size)
            
    def forward(self, x, tau=1, eps=1e-20, training=True):
        # We apply hidden layer projection from original embedding
        hw = F.tanh(self.h_w(x))
        
        # We apply second projection and softplus activation
        alpha = F.softplus(self.alpha_w(hw))
        # This rearranges alpha to be more intuitively BATCH_SIZE X M X K
        alpha = alpha.view(-1, self.M, self.K)
        # Take the log of all elements
        log_alpha = torch.log(alpha)
        if training:
            # We apply Gumbel-softmax trick to get code vectors d_w
            d_w = F.softmax((log_alpha + sample_gumbel(log_alpha.size())) / tau, dim=-1)
            # Find argmax of all d_w vectors
            _, ind = d_w.max(dim=-1)
        if not training:
            _, ind = alpha.max(dim=-1)
            # Allows us when not training to convert soft vector to a hard, binarized one-hot encoding vector
            d_w = torch.zeros_like(alpha).scatter_(-1, ind.unsqueeze(2), 1.0)
        # d_w is now BATCH x M x K x 1
        d_w = d_w.unsqueeze(-1)
        return d_w, ind

class Decoder(nn.Module):
    '''
    The decoder receives d_w as input from the encoder, and outputs the embedding generated by this code.
    It stores a set of source dictionaries, represented by A, and computes the proper embedding from a summation
    of M matrix-vector products.

    INPUT SHAPE: BATCH_SIZE X M X K X 1
    OUTPUT SHAPE: BATCH_SIZE X EMBEDDING_DIM
    '''
    def __init__(self, M, K, output_size, emb_size, position=False):
        super(Decoder, self).__init__()
        self.output_size = output_size
        self.K = K
        self.M = M
        self.emb_size = emb_size

        # Contains source dictionaries for computing embedding given codes
        self.A = Source_Dictionary(M, output_size, K)
        
        self.final = nn.Linear(self.emb_size, self.emb_size)
        self.position = position
        if position:
            # This linear layer maps to latent hidden representation
            self.c_e = nn.Linear(self.emb_size * 2 + 768, self.emb_size)
        else:
            self.c_e = nn.Linear(self.emb_size * 2, self.emb_size)



    # Following the formula in the paper, performs multiplication and summation over the M matrix-vector products
    def forward(self, d_w, static):
        output = self.A(d_w) 
        codeapprox = output.reshape(output.shape[0], self.M * self.output_size)
        return self.c_e(torch.cat([codeapprox, static], dim = 1))


class Code_Learner(nn.Module):
    '''
    Our final autoencoder model structure used to train our encoded embeddings with an end-to-end network
    INPUT: baseline embeddings BATCH_SIZE X EMB_DIMENSIONS
    OUTPUT: BATCH_SIZE X EMBEDDING_DIM (final encoding representation)
    '''
    def __init__(self,emb_size, M, K, hidden_size = None, position = False):
        super(Code_Learner, self).__init__()
        # Initialize encoder and decoder components
        self.encoder = Encoder(emb_size, M, K, hidden_size, position)
        self.decoder = Decoder(M = M, K = K, output_size = emb_size // M, emb_size = emb_size, position=position)
        self.emb_size = emb_size
        self.position = position

    # Set up basic, normal encoder-decoder structure
    def forward(self, x, tau=1, eps=1e-20, training=True):
        
        d_w, _ = self.encoder(x, tau, eps, training)
        comp_emb = self.decoder(d_w, x[:,self.emb_size:])
        return comp_emb


class Source_Dictionary(nn.Module):
    r"""I basically modified the source code for the nn.Linear() class
        Removed bias, and the weights are of dimension M X EMBEDDING_SIZE X K
        INPUT: BATCH_SIZE X M X K X 1

        OUTPUT:BATCH_SIZE X K X 1
    """

    def __init__(self, M, emb_size, K):
        super(Source_Dictionary, self).__init__()
        # The weight of the dictionary is the set of M dictionaries of size EMB X K
        self.weight = Parameter(torch.Tensor(M, emb_size, K))
        self.reset_parameters()

    # Initialize parameters of Source_Dictionary
    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)

    # Operation necessary for proper batch matrix multiplication
    def forward(self, input):
        result = torch.matmul(self.weight, input)
        return result.squeeze(-1)
