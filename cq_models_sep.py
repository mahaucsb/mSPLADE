'''
Script  for  product CQ
'''
import torch
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.parameter import Parameter


# Quick utility function to sample from the Gumbel-distribution: -Log(-Log(Uniform)), eps to avoid numerical errors
def sample_gumbel(input_size, eps=1e-20):
    unif = torch.rand(input_size)

    return -torch.log(-torch.log(unif + eps) + eps)

class Encoder(nn.Module):
    def __init__(self, emb_size, hidden_size= None):
        super(Encoder, self).__init__()
        # input is cat[non-static, static]
        self.emb_size = emb_size
        # If not otherwise specified, use hidden_size indicated by paper
        if not hidden_size:
            #hidden_size = int(M * K / 2)
            hidden_size = 128
        self.get_residual = nn.Linear(self.emb_size * 2, hidden_size)

            
    def forward(self, x):
        # We apply hidden layer projection from original embedding
        hw = F.tanh(self.get_residual(x))
        return hw

class Coder(nn.Module):
    def __init__(self, M, K, hidden_size = None):
        super(Coder, self).__init__()
        # input is cat[non-static, static]
        self.K = K
        self.M = M
        # If not otherwise specified, use hidden_size indicated by paper
        if not hidden_size:
            #hidden_size = int(M * K / 2)
            hidden_size = 128
        
        self.alpha_w = nn.Linear(int(M * K // 2),int(M * K))
        self.h_w = nn.Linear(hidden_size, int(M * K // 2))
            
    def forward(self, x, tau=1, eps=1e-20, training=True):
        # We apply hidden layer projection from original embedding
        hw = F.tanh(self.h_w(x))
        
        # We apply second projection and softplus activation
        alpha = F.softplus(self.alpha_w(hw))
        # This rearranges alpha to be more intuitively BATCH_SIZE X M X K
        alpha = alpha.view(-1, self.M, self.K)
        # Take the log of all elements
        log_alpha = torch.log(alpha)
        if training:
            # We apply Gumbel-softmax trick to get code vectors d_w
            d_w = F.softmax((log_alpha + sample_gumbel(log_alpha.size()).to(log_alpha.device)) / tau, dim=-1)
            # Find argmax of all d_w vectors
            _, ind = d_w.max(dim=-1)
        if not training:
            _, ind = alpha.max(dim=-1)
            # Allows us when not training to convert soft vector to a hard, binarized one-hot encoding vector
            d_w = torch.zeros_like(alpha).scatter_(-1, ind.unsqueeze(2), 1.0)
        # d_w is now BATCH x M x K x 1
        d_w = d_w.unsqueeze(-1)
        return d_w, ind

class Reader(nn.Module):
    '''
    The decoder receives d_w as input from the encoder, and outputs the embedding generated by this code.
    It stores a set of source dictionaries, represented by A, and computes the proper embedding from a summation
    of M matrix-vector products.

    INPUT SHAPE: BATCH_SIZE X M X K X 1
    OUTPUT SHAPE: BATCH_SIZE X EMBEDDING_DIM
    '''
    def __init__(self, M, K, output_size):
        super(Reader, self).__init__()
        self.K = K
        self.M = M
        self.output_size = output_size

        # Contains source dictionaries for computing embedding given codes
        self.A = Source_Dictionary(M, output_size // M, K)
        
    # Following the formula in the paper, performs multiplication and summation over the M matrix-vector products
    def forward(self, d_w):
        output = self.A(d_w) 
        codeapprox = output.reshape(output.shape[0], self.output_size)
        return codeapprox

class Decoder(nn.Module):
    def __init__(self, output_size, emb_size):
        super(Decoder, self).__init__()
        self.emb_size = emb_size
        self.final = nn.Linear(self.emb_size, self.emb_size)
        
        self.c_e = nn.Linear(self.emb_size + output_size, self.emb_size)

    # Following the formula in the paper, performs multiplication and summation over the M matrix-vector products
    def forward(self, codeapprox, static):
        output = F.tanh(self.c_e(torch.cat([codeapprox, static], dim = 1)))
        output = self.final(output)
        return output



class Source_Dictionary(nn.Module):
    r"""I basically modified the source code for the nn.Linear() class
        Removed bias, and the weights are of dimension M X EMBEDDING_SIZE X K
        INPUT: BATCH_SIZE X M X K X 1

        OUTPUT:BATCH_SIZE X K X 1
    """

    def __init__(self, M, emb_size, K):
        super(Source_Dictionary, self).__init__()
        # The weight of the dictionary is the set of M dictionaries of size EMB X K
        self.weight = Parameter(torch.Tensor(M, emb_size, K))
        self.reset_parameters()

    # Initialize parameters of Source_Dictionary
    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(1))
        self.weight.data.uniform_(-stdv, stdv)

    # Operation necessary for proper batch matrix multiplication
    def forward(self, input):
        result = torch.matmul(self.weight, input)
        return result.squeeze(-1)
